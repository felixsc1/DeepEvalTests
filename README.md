# DeepEval Learning Repository

Collection of notebooks and scripts to learn about:
- Generating synthetic test datasets for RAG systems (using DeepEval Synthesizer)
- Filtering documents and goldens to generate high quality datasets
- Evaluating RAG systems using DeepEval, with various metrics. 
- Building more complex RAG systems with LangChain. Perform quantitative comparisons of their performance with DeepEval.


## Dataset
As a starting basis I use the scraped emails/posts/quotes obtained from my other repository: https://github.com/felixsc1/SatoshiBot
The latest version creates a folder full of .txt documents (and their medatata in json files). Use `path_updater.py` to fix hardcoded file paths in the files when using a different machine.

`filter_documents.py` is used to filter the documents to remove duplicates and low-quality documents using Ollama embeddings and cosine similarity (adjust model in the file as needed). This reduces the time and cost for generating goldens with Synthesizer.


## Generating Goldens
Generation of initial goldens (QA pairs) in `1_deepeval_synthesizer.ipynb`
- I noticed that Ollama models are insufficient (at least <8b parameters) for synthesizer. Also, getting timeout errors with gpt-5. Most reliable and affordable model is gpt-4o-mini.
- Had to wrap the entire Synthesizer pipeline in a complicated structure to handle timeouts, retries, and errors. This is called from `robust_synthesizer.py`. Generated by Grok, essentially it keeps updating synthesis_progress.json to keep track of already processed documents. In case everything crashes (which happens), can restart the kernel and rerun without loss.
- Also check out the custom logic to expand the goldens with metadata (here the URL of the source document). By default, Synthesizer only takes in lists of strings, or lists of raw documents.

**Problem with the default approach**: Synthesizer's generate_goldens_from_docs() treats every document independently (makes it easier to validate that golden question matches specific context). But production RAG systems retrieve multiple documents (e.g. top-k = 3), so ideally we should also generate goldens that are based on multiple documents.
`1_optional_multicontext_document_generation.ipynb` shows a way to achieve this.
- Uses Silhouette Score to find clusters that are semantically coherent and different from each other.
- Uses a loop to find the optimal number of clusters, nudged towards ~3 documents per cluster.
- Uses synthesizer.generate_goldens_from_contexts() skips the context construction step (we did this above), but uses the same logic and quality filtering to generate goldens.

**Post processing of Goldens**

Done in `2_post_process_goldens.ipynb` (mostly AI generated code) for some visualizations, and `duplicate_analysis.py` for automated filtering.

- Synthesizer generates goldens with a score for input quality unter `['additional_metadata']['synthetic_input_quality']`. It does not filter out anything yet.
- Initial step is to figure out a good quality cutoff, to still retain a decent amount of goldens, e.g. 0.8.
- Also, Synthesizer treats every document completely independently, so there may still be duplicate questions generated from different similar documents.
- Thus we re-use the logic from `filter_documents.py` (to remove duplicate raw documents), to remove also duplicate questions. 
- Matplotlib is used to visualize the distribution of input quality scores and similarities between questions. Also shows how many goldens remain at different cutoffs.

Final result is a file like `filtered_goldens_quality_0.8_similarity_0.8.json` indicating the cutoffs.
