{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Patch OpenAI for tracing BEFORE any deepeval imports\n",
    "import openai\n",
    "from deepeval.openai.patch import patch_openai\n",
    "patch_openai(openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa70daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "LLM_MODEL = \"llama3.1:8b\"\n",
    "EMBEDDING_MODEL = \"mxbai-embed-large:335m\"\n",
    "\n",
    "RAW_FILES = r\"C:\\GitRepos\\LangChainCourse\\documentation_assistant\\raw_documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a3e9c",
   "metadata": {},
   "source": [
    "# 1. Set up a basic RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb305459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:00<00:00, 5932.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 617 documents and created 1067 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings:   0%|          | 0/1067 [00:00<?, ?it/s]Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Creating embeddings: 100%|██████████| 1067/1067 [00:16<00:00, 65.39it/s]\n",
      "C:\\Users\\felix\\AppData\\Local\\Temp\\ipykernel_8556\\3975680944.py:59: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=LLM_MODEL)\n",
      "C:\\Users\\felix\\AppData\\Local\\Temp\\ipykernel_8556\\3975680944.py:70: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain.run(query)\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and persisted in 16.32 seconds\n",
      "Query: What did Satoshi say about quantum computing? Is it a threat?\n",
      "Answer: I don't know. The provided sources do not mention anything related to quantum computing or its potential threat to Bitcoin. They primarily discuss the concept of proof-of-work, the Byzantine Generals' Problem, and the security features of Bitcoin.\n"
     ]
    }
   ],
   "source": [
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# Initialize Ollama embeddings\n",
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "\n",
    "# Check if vector store already exists\n",
    "if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
    "    print(\"Loading existing vector store...\")\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"rag_collection\"\n",
    "    )\n",
    "    print(f\"Loaded existing vector store with {vector_store._collection.count()} documents\")\n",
    "else:\n",
    "    print(\"Creating new vector store...\")\n",
    "    # Load documents from directory with subfolders\n",
    "    loader = DirectoryLoader(\n",
    "        path=RAW_FILES,\n",
    "        glob=\"**/*.txt\",  # This will find all .txt files in all subdirectories\n",
    "        loader_cls=TextLoader,  # Use TextLoader for .txt files\n",
    "        show_progress=True  # Optional: shows loading progress\n",
    "    )\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents and created {len(chunks)} chunks\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Create vector store with progress tracking\n",
    "    with tqdm(total=len(chunks), desc=\"Creating embeddings\") as pbar:\n",
    "        # We'll create the vector store in batches to show progress\n",
    "        batch_size = 100\n",
    "        vector_store = None\n",
    "\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[i:i + batch_size]\n",
    "            if vector_store is None:\n",
    "                # First batch - create the vector store\n",
    "                vector_store = Chroma.from_documents(\n",
    "                    documents=batch,\n",
    "                    embedding=embeddings,\n",
    "                    collection_name=\"rag_collection\",\n",
    "                    persist_directory=persist_directory\n",
    "                )\n",
    "            else:\n",
    "                # Subsequent batches - add to existing store\n",
    "                vector_store.add_documents(batch)\n",
    "\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Vector store created and persisted in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = Ollama(model=LLM_MODEL)\n",
    "\n",
    "# Set up RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3})  # Retrieve top 4 documents\n",
    ")\n",
    "\n",
    "# Test query\n",
    "query = \"What did Satoshi say about quantum computing? Is it a threat?\"\n",
    "result = qa_chain.run(query)\n",
    "print(\"Query:\", query)\n",
    "print(\"Answer:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd54693",
   "metadata": {},
   "source": [
    "# 2 Testing the RAG pipeline with Goldens\n",
    "Dataset obtained from notebooks 1 and 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e75ec",
   "metadata": {},
   "source": [
    "### Loading goldens, preparing test cases, generating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1d39de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 181 goldens from filtered_goldens_quality_0.8_similarity_0.85.json\n",
      "Prepared 5 LLMTestCase objects\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, ContextualRecallMetric, ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "goldens_file = 'filtered_goldens_quality_0.8_similarity_0.85.json'\n",
    "\n",
    "\n",
    "def load_and_prepare_goldens(json_file_path, max_samples=None):\n",
    "    \"\"\"Load goldens and transform to deepeval format\"\"\"\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        goldens_data = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(goldens_data)} goldens from {json_file_path}\")\n",
    "\n",
    "    # Transform to deepeval LLMTestCase objects\n",
    "    # Your JSON: {\"input\": query, \"expected_output\": ground_truth, \"context\": [contexts]}\n",
    "    # LLMTestCase expects: input, expected_output, context (as list of strings)\n",
    "    test_cases = []\n",
    "    data_to_process = goldens_data[:max_samples] if max_samples else goldens_data\n",
    "\n",
    "    for item in data_to_process:\n",
    "        test_case = LLMTestCase(\n",
    "            input=item[\"input\"],\n",
    "            expected_output=item[\"expected_output\"],\n",
    "            context=item[\"context\"]  # This should be a list of strings\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "\n",
    "    print(f\"Prepared {len(test_cases)} LLMTestCase objects\")\n",
    "    return test_cases\n",
    "\n",
    "def generate_predictions(test_cases, qa_chain, vector_store):\n",
    "    \"\"\"Generate predictions for each test case using RAG pipeline and populate actual_output/retrieval_context\"\"\"\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"Generating prediction {i+1}/{len(test_cases)}: {test_case.input[:60]}...\")\n",
    "\n",
    "        try:\n",
    "            # Run your RAG pipeline to get answer\n",
    "            answer = qa_chain.run(test_case.input)\n",
    "\n",
    "            # Get actually retrieved contexts (what your RAG system retrieved)\n",
    "            retrieved_docs = vector_store.as_retriever().get_relevant_documents(test_case.input)\n",
    "            retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n",
    "\n",
    "            # Populate the test case with actual output and retrieval context\n",
    "            test_case.actual_output = answer\n",
    "            test_case.retrieval_context = retrieved_contexts\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating prediction for query {i+1}: {e}\")\n",
    "            test_case.actual_output = f\"Error: {str(e)}\"\n",
    "            test_case.retrieval_context = []\n",
    "\n",
    "    print(f\"Generated predictions for {len(test_cases)} test cases\")\n",
    "    return test_cases\n",
    "\n",
    "test_cases = load_and_prepare_goldens(goldens_file, max_samples=5)  # For debugging, dont just run hundreds yet, start small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2866a4e",
   "metadata": {},
   "source": [
    "### Running evaluation \n",
    "Strongly recommended to use OpenAI LLM for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb21c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction 1/5: What is the significance of the Bitcoin v0.1 release?...\n",
      "Generating prediction 2/5: What is the main idea behind Bitcoin?...\n",
      "Generating prediction 3/5: What is the main idea behind Satoshi Nakamoto's proposal?...\n",
      "Generating prediction 4/5: What is the main concern regarding CPU power in the Bitcoin ...\n",
      "Generating prediction 5/5: What happens if a broadcast transaction does not reach all n...\n",
      "Generated predictions for 5 test cases\n"
     ]
    }
   ],
   "source": [
    "# NOTE: You need to have vector_store and qa_chain defined/imported\n",
    "test_cases = generate_predictions(test_cases, qa_chain, vector_store)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbb23290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Recall Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Recall Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354b0cee33aa4d2a903c09d41f2fe4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No test cases found, please try again.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "No test cases found, please try again.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "test_results=[] confident_link=None\n"
     ]
    }
   ],
   "source": [
    "from deepeval.models import GPTModel\n",
    "\n",
    "llm_gpt = GPTModel(\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "\n",
    "def run_evaluation(test_cases):\n",
    "  \"\"\"Run deepeval evaluation\"\"\"\n",
    "  metrics = [\n",
    "      AnswerRelevancyMetric(),\n",
    "      FaithfulnessMetric(),\n",
    "      ContextualRecallMetric(),\n",
    "      ContextualPrecisionMetric()\n",
    "  ]\n",
    "\n",
    "  print(\"Running evaluation...\")\n",
    "  results = evaluate(\n",
    "      test_cases=test_cases,\n",
    "      metrics=metrics\n",
    "  )\n",
    "\n",
    "  print(\"\\n\" + \"=\"*50)\n",
    "  print(\"EVALUATION RESULTS\")\n",
    "  print(\"=\"*50)\n",
    "  print(results)\n",
    "  return results\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "results = run_evaluation(test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd4278e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[], confident_link=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
