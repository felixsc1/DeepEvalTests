{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "839d8c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode encoding setup complete.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Patch OpenAI for tracing BEFORE any deepeval imports\n",
    "import openai\n",
    "from deepeval.openai.patch import patch_openai\n",
    "patch_openai(openai)\n",
    "\n",
    "# Fix Unicode encoding issues on Windows\n",
    "import os\n",
    "if os.name == 'nt':\n",
    "    # Set environment variable for UTF-8 encoding\n",
    "    os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "    \n",
    "    # Try to set UTF-8 locale\n",
    "    import locale\n",
    "    try:\n",
    "        # Try different UTF-8 locale names\n",
    "        for loc in ['en_US.UTF-8', 'C.UTF-8', 'UTF-8']:\n",
    "            try:\n",
    "                locale.setlocale(locale.LC_ALL, loc)\n",
    "                break\n",
    "            except locale.Error:\n",
    "                continue\n",
    "        else:\n",
    "            print(\"Warning: Could not set UTF-8 locale. Unicode display may be limited.\")\n",
    "    except:\n",
    "        print(\"Warning: Locale setting failed. Unicode display may be limited.\")\n",
    "\n",
    "print(\"Unicode encoding setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa70daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "LLM_MODEL = \"llama3.1:8b\"\n",
    "EMBEDDING_MODEL = \"mxbai-embed-large:335m\"\n",
    "\n",
    "RAW_FILES = r\"C:\\GitRepos\\LangChainCourse\\documentation_assistant\\raw_documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a3e9c",
   "metadata": {},
   "source": [
    "# 1. Set up a basic RAG pipeline\n",
    "\n",
    "Plan is to build the best possible RAG architecture with local Ollama models. But use OpenAI for evaluation, as evaluation is the much harder task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb305459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\AppData\\Local\\Temp\\ipykernel_9232\\3829148254.py:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "C:\\Users\\felix\\AppData\\Local\\Temp\\ipykernel_9232\\3829148254.py:58: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=LLM_MODEL)\n",
      "C:\\Users\\felix\\AppData\\Local\\Temp\\ipykernel_9232\\3829148254.py:69: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing vector store with 1067 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What did Satoshi say about quantum computing? Is it a threat?\n",
      "Answer: The answer is not directly stated in the provided sources. However, I can look for any mentions of quantum computing by Satoshi Nakamoto.\n",
      "\n",
      "In one email (Source: https://satoshi.nakamotoinstitute.org/emails/cryptography/11/), there's a discussion about the Byzantine Generals problem and its solution using proof-of-work chain. Quantum computing is not mentioned in this context.\n",
      "\n",
      "In another post on BitcoinTalk (Source: https://satoshi.nakamotoinstitute.org/posts/bitcointalk/418/), Satoshi Nakamoto discusses the concept of an alert system for the Bitcoin network but doesn't mention quantum computing.\n",
      "\n",
      "Unfortunately, I don't have any information about Satoshi's views on quantum computing. If you're looking for a more detailed answer or confirmation from other sources, I recommend searching further.\n"
     ]
    }
   ],
   "source": [
    "persist_directory = \"./chroma_db\"\n",
    "top_k_retrieved_documents = 3\n",
    "\n",
    "# Initialize Ollama embeddings\n",
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "\n",
    "# Check if vector store already exists\n",
    "if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
    "    print(\"Loading existing vector store...\")\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"rag_collection\"\n",
    "    )\n",
    "    print(f\"Loaded existing vector store with {vector_store._collection.count()} documents\")\n",
    "else:\n",
    "    print(\"Creating new vector store...\")\n",
    "    # Load documents from directory with subfolders\n",
    "    loader = DirectoryLoader(\n",
    "        path=RAW_FILES,\n",
    "        glob=\"**/*.txt\",  # This will find all .txt files in all subdirectories\n",
    "        loader_cls=TextLoader,  # Use TextLoader for .txt files\n",
    "        show_progress=True  # Optional: shows loading progress\n",
    "    )\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents and created {len(chunks)} chunks\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Create vector store with progress tracking\n",
    "    with tqdm(total=len(chunks), desc=\"Creating embeddings\") as pbar:\n",
    "        # We'll create the vector store in batches to show progress\n",
    "        batch_size = 100\n",
    "        vector_store = None\n",
    "\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[i:i + batch_size]\n",
    "            if vector_store is None:\n",
    "                # First batch - create the vector store\n",
    "                vector_store = Chroma.from_documents(\n",
    "                    documents=batch,\n",
    "                    embedding=embeddings,\n",
    "                    collection_name=\"rag_collection\",\n",
    "                    persist_directory=persist_directory\n",
    "                )\n",
    "            else:\n",
    "                # Subsequent batches - add to existing store\n",
    "                vector_store.add_documents(batch)\n",
    "\n",
    "            pbar.update(len(batch))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Vector store created and persisted in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = Ollama(model=LLM_MODEL)\n",
    "\n",
    "# Set up RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": top_k_retrieved_documents})  # Retrieve top 4 documents\n",
    ")\n",
    "\n",
    "# Test query\n",
    "query = \"What did Satoshi say about quantum computing? Is it a threat?\"\n",
    "result = qa_chain.run(query)\n",
    "print(\"Query:\", query)\n",
    "print(\"Answer:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd54693",
   "metadata": {},
   "source": [
    "# 2 Testing the RAG pipeline with Goldens\n",
    "Dataset obtained from notebooks 1 and 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e75ec",
   "metadata": {},
   "source": [
    "### Loading goldens, preparing test cases, generating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d39de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 181 goldens from filtered_goldens_quality_0.8_similarity_0.85.json\n",
      "Prepared 5 LLMTestCase objects\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, ContextualRecallMetric, ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "goldens_file = 'filtered_goldens_quality_0.8_similarity_0.85.json'\n",
    "\n",
    "\n",
    "def load_and_prepare_goldens(json_file_path, max_samples=None):\n",
    "    \"\"\"Load goldens and transform to deepeval format\"\"\"\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        goldens_data = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(goldens_data)} goldens from {json_file_path}\")\n",
    "\n",
    "    # Transform to deepeval LLMTestCase objects\n",
    "    # Your JSON: {\"input\": query, \"expected_output\": ground_truth, \"context\": [contexts]}\n",
    "    # LLMTestCase expects: input, expected_output, context (as list of strings)\n",
    "    test_cases = []\n",
    "    data_to_process = goldens_data[:max_samples] if max_samples else goldens_data\n",
    "\n",
    "    for item in data_to_process:\n",
    "        test_case = LLMTestCase(\n",
    "            input=item[\"input\"],\n",
    "            expected_output=item[\"expected_output\"],\n",
    "            context=item[\"context\"]  # In Confident-AI will show up as Context. The ideal document to be found.\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "\n",
    "    print(f\"Prepared {len(test_cases)} LLMTestCase objects\")\n",
    "    return test_cases\n",
    "\n",
    "\n",
    "# Optional: from deepeval.tracing import observe, then place the @observe()  decorator above this function:\n",
    "def generate_predictions(test_cases, qa_chain, vector_store):\n",
    "    \"\"\"Generate predictions for each test case using RAG pipeline and populate actual_output/retrieval_context\"\"\"\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"Generating prediction {i+1}/{len(test_cases)}: {test_case.input[:60]}...\")\n",
    "\n",
    "        try:\n",
    "            # Run your RAG pipeline to get answer\n",
    "            answer = qa_chain.run(test_case.input)\n",
    "\n",
    "            # Get actually retrieved contexts (what your RAG system retrieved, important to be consistent here. i.e. the same top-k)\n",
    "            # This is then what shows up in Confident-AI under \"Retrieved Context\"\n",
    "            retrieved_docs = vector_store.as_retriever(search_kwargs={\"k\": top_k_retrieved_documents}).get_relevant_documents(test_case.input)\n",
    "            retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n",
    "\n",
    "            # Populate the test case with actual output and retrieval context\n",
    "            test_case.actual_output = answer\n",
    "            test_case.retrieval_context = retrieved_contexts\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating prediction for query {i+1}: {e}\")\n",
    "            test_case.actual_output = f\"Error: {str(e)}\"\n",
    "            test_case.retrieval_context = []\n",
    "\n",
    "    print(f\"Generated predictions for {len(test_cases)} test cases\")\n",
    "    return test_cases\n",
    "\n",
    "test_cases = load_and_prepare_goldens(goldens_file, max_samples=5)  # For debugging, dont just run hundreds yet, start small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11437ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction 1/5: What is the significance of the Bitcoin v0.1 release?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\AppData\\Local\\Temp\\ipykernel_9232\\3015986749.py:44: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = vector_store.as_retriever().get_relevant_documents(test_case.input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction 2/5: What is the main idea behind Bitcoin?...\n",
      "Generating prediction 3/5: What is the main idea behind Satoshi Nakamoto's proposal?...\n",
      "Generating prediction 4/5: What is the main concern regarding CPU power in the Bitcoin ...\n",
      "Generating prediction 5/5: What happens if a broadcast transaction does not reach all n...\n",
      "Generated predictions for 5 test cases\n"
     ]
    }
   ],
   "source": [
    "test_cases = generate_predictions(test_cases, qa_chain, vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2866a4e",
   "metadata": {},
   "source": [
    "### Running evaluation \n",
    "Strongly recommended to use OpenAI LLM for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbb23290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 5 test cases with 4 metrics...\n",
      "Using batch size: 2, delay between batches: 2s\n",
      "\n",
      "Processing batch 1/3 (test cases 1-2)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Recall Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Recall Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9417029b92a443d2881bdf1d5182b89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âœ… Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because the response directly addresses the main idea behind Bitcoin without any irrelevant statements., error: None)\n",
      "  - âœ… Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because there are no contradictions present, indicating that the actual output aligns perfectly with the retrieval context., error: None)\n",
      "  - âœ… Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because all sentences in the expected output are directly supported by the information in node(s) in retrieval context, clearly articulating the core concepts of Bitcoin without any contradictions., error: None)\n",
      "  - âœ… Contextual Precision (score: 0.8333333333333333, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 0.83 because while the relevant nodes are well-ranked, there are still some irrelevant nodes that are positioned between them. For instance, the second node (rank 2) discusses Bitcoin's relation to previous proposals but fails to address the main idea, which detracts from the overall precision. However, the first node (rank 1) effectively explains Bitcoin's decentralized nature, and the third node (rank 3) provides a clear definition of Bitcoin as a digital currency, both of which contribute positively to the score., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the main idea behind Bitcoin?\n",
      "  - actual output: The main idea behind Bitcoin is to create a decentralized electronic cash system that eliminates the need for trust in traditional currencies by using cryptographic proof instead.\n",
      "  - expected output: The main idea behind Bitcoin is to create a peer-to-peer electronic cash system that allows online payments to be sent directly between parties without the need for a trusted third party, such as a financial institution. It prevents double-spending through a decentralized network that timestamps transactions and maintains a secure, immutable record using proof-of-work. This system enables participants to remain anonymous while ensuring the integrity of transactions.\n",
      "  - context: ['Source: https://satoshi.nakamotoinstitute.org/emails/cryptography/1/\\nFrom: | Satoshi Nakamoto | Subject: | Bitcoin P2P e-cash paper | Date: | October 31, 2008 at 18:10:00 UTC\\nI\\'ve been working on a new electronic cash system that\\'s fully\\npeer-to-peer, with no trusted third party.\\nThe paper is available at:\\nhttp://www.bitcoin.org/bitcoin.pdf\\nThe main properties:\\nDouble-spending is prevented with a peer-to-peer network.\\nNo mint or other trusted parties.\\nParticipants can be anonymous.\\nNew coins are made from Hashcash style proof-of-work.\\nThe proof-of-work for new coin generation also powers the\\nnetwork to prevent double-spending.\\nBitcoin: A Peer-to-Peer Electronic Cash System\\nAbstract.  A purely peer-to-peer version of electronic cash would\\nallow online payments to be sent directly from one party to another\\nwithout the burdens of going through a financial institution.\\nDigital signatures provide part of the solution, but the main\\nbenefits are lost if a trusted party is still required to prevent\\ndouble-spending.  We propose a solution to the double-spending\\nproblem using a peer-to-peer network.  The network timestamps\\ntransactions by hashing them into an ongoing chain of hash-based\\nproof-of-work, forming a record that cannot be changed without\\nredoing the proof-of-work.  The longest chain not only serves as\\nproof of the sequence of events witnessed, but proof that it came\\nfrom the largest pool of CPU power.  As long as honest nodes control\\nthe most CPU power on the network, they can generate the longest\\nchain and outpace any attackers.  The network itself requires\\nminimal structure.  Messages are broadcasted on a best effort basis,\\nand nodes can leave and rejoin the network at will, accepting the\\nlongest proof-of-work chain as proof of what happened while they\\nwere gone.\\nFull paper at:\\nhttp://www.bitcoin.org/bitcoin.pdf\\nSatoshi Nakamoto\\n---------------------------------------------------------------------\\nThe Cryptography Mailing List\\nUnsubscribe by sending \"unsubscribe cryptography\" to majordomo at metzdowd.com\\nBitcoin P2P e-cash paper\\nCryptography Mailing List']\n",
      "  - retrieval context: [\"I've developed a new open source P2P e-cash system called Bitcoin. It's completely decentralized, with no central server or trusted parties, because everything is based on crypto proof instead of trust. Give it a try, or take a look at the screenshots and design paper:Download Bitcoin v0.1 athttp://www.bitcoin.orgThe root problem with conventional currency is all the trust that's required to make it work. The central bank must be trusted not to debase the currency, but the history of fiat currencies is full of breaches of that trust. Banks must be trusted to hold our money and transfer it electronically, but they lend it out in waves of credit bubbles with barely a fraction in reserve. We have to trust them with our privacy, trust them not to let identity thieves drain our accounts. Their massive overhead costs make micropayments impossible.A generation ago, multi-user time-sharing computer systems had a similar problem. Before strong encryption, users had to rely on password protection to secure their\", 'Bitcoin is an implementation of Wei Dai\\'s b-money proposalhttp://weidai.com/bmoney.txton Cypherpunkshttp://en.wikipedia.org/wiki/Cypherpunksin 1998 and Nick Szabo\\'s Bitgold proposalhttp://unenumerated.blogspot.com/2005/12/bit-gold.htmlThe timing is strange, just as we are getting a rapid increase in 3rd party coverage after getting slashdotted. Ãƒâ€šÃ‚\\xa0I hope there\\'s not a big hurry to wrap the discussion and decide. Ãƒâ€šÃ‚\\xa0How long does Wikipedia typically leave a question like that open for comment?It would help to condense the article and make it less promotional sounding as soon as possible. Ãƒâ€šÃ‚\\xa0Just letting people know what it is, where it fits into the electronic money space, not trying to convince them that it\\'s good. Ãƒâ€šÃ‚\\xa0They probably want something that just generally identifies what it is, not tries to explain all about how it works.If you post inhttp://en.wikipedia.org/wiki/Wikipedia:Articles_for_deletion/Bitcoinplease don\\'t say \"yeah, but bitcoin is really important and special so the rules shouldn\\'t', \"Source: https://satoshi.nakamotoinstitute.org/emails/p2p-research/35/\\nFrom: | Satoshi Nakamoto | Subject: | [p2p-research] Bitcoin open source implementation of P2P currency | Date: | February 11, 2009 at 22:37:54 UTC\\nI've developed a new open source P2P e-cash system called Bitcoin.  It's\\ncompletely decentralized, with no central server or trusted parties,\\nbecause everything is based on crypto proof instead of trust.  Give it a\\ntry, or take a look at the screenshots and design paper:\\nDownload Bitcoin v0.1 at\\nhttp://www.bitcoin.org\\nThe root problem with conventional currency is all the trust that's\\nrequired to make it work.  The central bank must be trusted not to\\ndebase the currency, but the history of fiat currencies is full of\\nbreaches of that trust.  Banks must be trusted to hold our money and\\ntransfer it electronically, but they lend it out in waves of credit\\nbubbles with barely a fraction in reserve.  We have to trust them with\\nour privacy, trust them not to let identity thieves drain our accounts.\", \"Source: https://satoshi.nakamotoinstitute.org/posts/bitcointalk/168/\\nAnnouncing version 0.3 of Bitcoin, the P2P cryptocurrency! Ãƒâ€šÃ‚\\xa0Bitcoin is a digital currency using cryptography and a distributed network to replace the need for a trusted central server. Ãƒâ€šÃ‚\\xa0Escape the arbitrary inflation risk of centrally managed currencies! Ãƒâ€šÃ‚\\xa0Bitcoin's total circulation is limited to 21 million coins. Ãƒâ€šÃ‚\\xa0The coins are gradually released to the network's nodes based on the CPU power they contribute, so you can get a share of them by contributing your idle CPU time.What's new:- Command line and JSON-RPC control- Includes a daemon version without GUI- Transaction filter tabs- 20% faster hashing- Hashmeter performance display- Mac OS X version (thanks to Laszlo)- German, Dutch and Italian translations (thanks to DataWraith, Xunie and Joozero)Get it athttp://www.bitcoin.orgor read the forum to find out more.\\nBitcoin 0.3 released!\\nJuly 6, 2010 at 18:32:35 UTC\\nBitcoinTalk\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âœ… Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because the response directly addresses the significance of the Bitcoin v0.1 release without any irrelevant statements., error: None)\n",
      "  - âœ… Faithfulness (score: 0.75, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 0.75 because the actual output incorrectly states that the Bitcoin v0.1 release was announced on January 8, 2009, while it was actually released on that date., error: None)\n",
      "  - âœ… Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because every aspect of the expected output is directly supported by the relevant nodes in the retrieval context, highlighting the significance of the Bitcoin v0.1 release and its foundational role in the cryptocurrency ecosystem., error: None)\n",
      "  - âœ… Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because the first node provides essential information about the significance of the Bitcoin v0.1 release, describing it as 'a new electronic cash system that uses a peer-to-peer network to prevent double-spending,' which is crucial for understanding its importance. The subsequent nodes, ranked lower, are irrelevant as they focus on later versions or unrelated aspects, thus justifying their lower ranking., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the significance of the Bitcoin v0.1 release?\n",
      "  - actual output: The significance of the Bitcoin v0.1 release is that it was the first publicly released version of the Bitcoin software, announced by Satoshi Nakamoto on January 8, 2009. This release marked the beginning of the public availability of Bitcoin and paved the way for the development and growth of the cryptocurrency.\n",
      "  - expected output: The Bitcoin v0.1 release, announced on January 12, 2009, is significant as it marked the official launch of the Bitcoin software, introducing the first implementation of the cryptocurrency. This release laid the foundation for the Bitcoin network, enabling peer-to-peer transactions without the need for intermediaries, and set the stage for the development and growth of the entire blockchain ecosystem.\n",
      "  - context: ['Source: https://satoshi.nakamotoinstitute.org/emails/bitcoin-list/\\nEmails\\n[bitcoin-list] WelcomeDec 10, 2008, 17:00:23 UTC[bitcoin-list] Bitcoin v0.1.2 now availableJan 11, 2009, 22:32:18 UTC[bitcoin-list] Bitcoin v0.1 Alpha release notesJan 12, 2009, 20:20:47 UTC[bitcoin-list] Bitcoin v0.1.3Jan 12, 2009, 22:48:23 UTCRe: [bitcoin-list] Bitcoin v0.1 releasedJan 16, 2009, 18:35:32 UTCRe: [bitcoin-list] ProblemsJan 25, 2009, 16:45:25 UTC[bitcoin-list] Bitcoin v0.1.5 releasedFeb 4, 2009, 19:46:04 UTCRe: [bitcoin-list] Bitcoin v0.1.5 releasedFeb 22, 2009, 17:47:52 UTCRe: [bitcoin-list] Bitcoin v0.1.5 releasedMar 4, 2009, 16:59:12 UTCRe: [bitcoin-list] Does Bitcoin Crash in Windows?Oct 23, 2009, 23:57:51 UTC[bitcoin-list] Bitcoin 0.2 releasedDec 17, 2009, 06:52:09 UTC[bitcoin-list] Bitcoin 0.3 released!Jul 6, 2010, 21:53:53 UTC[bitcoin-list] Alert: upgrade to bitcoin 0.3.6Jul 30, 2010, 06:02:38 UTC[bitcoin-list] ALERT - we are investigating a problemAug 15, 2010, 20:38:33 UTC[bitcoin-list] Bitcoin 0.3.18 is releasedDec 8, 2010, 23:11:55 UTC[bitcoin-list] Bitcoin 0.3.19 is releasedDec 13, 2010, 16:12:09 UTC']\n",
      "  - retrieval context: [\"Source: https://satoshi.nakamotoinstitute.org/emails/cryptography/16/\\nFrom: | Satoshi Nakamoto | Subject: | Bitcoin v0.1 released | Date: | January 8, 2009 at 19:27:40 UTC\\nAnnouncing the first release of Bitcoin, a new electronic cash\\nsystem that uses a peer-to-peer network to prevent double-spending.\\nIt's completely decentralized with no server or central authority.\\nSee bitcoin.org for screenshots.\\nDownload link:\\nhttp://downloads.sourceforge.net/bitcoin/bitcoin-0.1.0.rar\\nWindows only for now.  Open source C++ code is included.\\n- Unpack the files into a directory\\n- Run BITCOIN.EXE\\n- It automatically connects to other nodes\\nIf you can keep a node running that accepts incoming connections,\\nyou'll really be helping the network a lot.  Port 8333 on your\\nfirewall needs to be open to receive incoming connections.\\nThe software is still alpha and experimental.  There's no guarantee\\nthe system's state won't have to be restarted at some point if it\\nbecomes necessary, although I've done everything I can to build in\", '[bitcoin-list] WelcomeDec 10, 2008, 17:00:23 UTC[bitcoin-list] Bitcoin v0.1.2 now availableJan 11, 2009, 22:32:18 UTC[bitcoin-list] Bitcoin v0.1 Alpha release notesJan 12, 2009, 20:20:47 UTC[bitcoin-list] Bitcoin v0.1.3Jan 12, 2009, 22:48:23 UTCRe: [bitcoin-list] Bitcoin v0.1 releasedJan 16, 2009, 18:35:32 UTCRe: [bitcoin-list] ProblemsJan 25, 2009, 16:45:25 UTC[bitcoin-list] Bitcoin v0.1.5 releasedFeb 4, 2009, 19:46:04 UTCRe: [bitcoin-list] Bitcoin v0.1.5 releasedFeb 22, 2009, 17:47:52 UTCRe: [bitcoin-list] Bitcoin v0.1.5 releasedMar 4, 2009, 16:59:12 UTCRe: [bitcoin-list] Does Bitcoin Crash in Windows?Oct 23, 2009, 23:57:51 UTC[bitcoin-list] Bitcoin 0.2 releasedDec 17, 2009, 06:52:09 UTC[bitcoin-list] Bitcoin 0.3 released!Jul 6, 2010, 21:53:53 UTC[bitcoin-list] Alert: upgrade to bitcoin 0.3.6Jul 30, 2010, 06:02:38 UTC[bitcoin-list] ALERT - we are investigating a problemAug 15, 2010, 20:38:33 UTC[bitcoin-list] Bitcoin 0.3.18 is releasedDec 8, 2010, 23:11:55 UTC[bitcoin-list] Bitcoin 0.3.19 is releasedDec', \"Source: https://satoshi.nakamotoinstitute.org/posts/bitcointalk/157/\\nBut 1.0 sounds like the first release.Ãƒâ€šÃ‚\\xa0 For some things newness is a virtue but for this type of software, maturity and stability are important.Ãƒâ€šÃ‚\\xa0 I don't want to put my money in something that's 1.0.Ãƒâ€šÃ‚\\xa0 1.0 might be more interesting for a moment, but after that we're still 1.0 and everyone who comes along thinks we just started.Ãƒâ€šÃ‚\\xa0 This is the third major release and 1.3 reflects that development history.Ãƒâ€šÃ‚\\xa0 (0.1, 0.2, 1.3)\\nRe: Beta?\\nJune 27, 2010 at 12:43:50 UTC\\nBitcoinTalk\", \"Source: https://satoshi.nakamotoinstitute.org/posts/bitcointalk/168/\\nAnnouncing version 0.3 of Bitcoin, the P2P cryptocurrency! Ãƒâ€šÃ‚\\xa0Bitcoin is a digital currency using cryptography and a distributed network to replace the need for a trusted central server. Ãƒâ€šÃ‚\\xa0Escape the arbitrary inflation risk of centrally managed currencies! Ãƒâ€šÃ‚\\xa0Bitcoin's total circulation is limited to 21 million coins. Ãƒâ€šÃ‚\\xa0The coins are gradually released to the network's nodes based on the CPU power they contribute, so you can get a share of them by contributing your idle CPU time.What's new:- Command line and JSON-RPC control- Includes a daemon version without GUI- Transaction filter tabs- 20% faster hashing- Hashmeter performance display- Mac OS X version (thanks to Laszlo)- German, Dutch and Italian translations (thanks to DataWraith, Xunie and Joozero)Get it athttp://www.bitcoin.orgor read the forum to find out more.\\nBitcoin 0.3 released!\\nJuly 6, 2010 at 18:32:35 UTC\\nBitcoinTalk\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "Contextual Recall: 100.00% pass rate\n",
      "Contextual Precision: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">âœ“</span> Done ðŸŽ‰! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w17cu05sahkynm7iu5lmz/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w17cu05sahkynm7iu5lmz/compa</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w17cu05sahkynm7iu5lmz/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">re-test-results</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141mâœ“\u001b[0m Done ðŸŽ‰! View results on \n",
       "\u001b]8;id=822545;https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w17cu05sahkynm7iu5lmz/compare-test-results\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w17cu05sahkynm7iu5lmz/compa\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=822545;https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w17cu05sahkynm7iu5lmz/compare-test-results\u001b\\\u001b[4;94mre-test-results\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 2 seconds before next batch...\n",
      "\n",
      "Processing batch 2/3 (test cases 3-4)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Recall Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Recall Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fc5cf2cc094b38a5f22f4162fd040b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âœ… Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because the response directly addresses the main concern regarding CPU power in the Bitcoin network without any irrelevant statements., error: None)\n",
      "  - âœ… Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because there are no contradictions present, indicating that the actual output aligns perfectly with the retrieval context., error: None)\n",
      "  - âŒ Contextual Recall (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 0.00 because none of the sentences in the expected output can be attributed to any node(s) in the retrieval context, as they all introduce concepts not present in the context., error: None)\n",
      "  - âŒ Contextual Precision (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 0.00 because all nodes in the retrieval contexts are irrelevant to the main concern regarding CPU power in the Bitcoin network. The first node ranks highest but states, \"The first document discusses CPU priority in Bitcoin but does not address concerns about CPU power being controlled by bad actors,\" indicating a lack of relevance. Similarly, the second node mentions, \"The second document talks about sharing CPU power between networks but does not mention the risks of bad actors overpowering the Bitcoin network,\" and the third node states, \"The third document is about runaway CPU usage and does not relate to the main concern regarding CPU power in the Bitcoin network.\" Lastly, the fourth node notes, \"The fourth document addresses thread priority fixes but does not discuss the security implications of CPU power in the Bitcoin network.\" Since all nodes fail to address the input question, the score remains at 0.00., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the main concern regarding CPU power in the Bitcoin network?\n",
      "  - actual output: The main concern regarding CPU power in the Bitcoin network is that if multiple networks with competing proof-of-work systems are to coexist, they may gang up on each other and fragment the overall CPU power available for mining. This could potentially undermine the security of the network.\n",
      "\n",
      "However, one proposed solution is for multiple networks to share their combined CPU power by allowing miners to search for proof-of-work for all networks simultaneously. This would enable the total CPU power available for mining to increase, rather than being fragmented among competing networks.\n",
      "  - expected output: The main concern regarding CPU power in the Bitcoin network is that bad actors can control significant computational resources, such as large zombie farms, which could potentially overpower the network. For the network to remain secure, honest nodes must collectively have more CPU power than any single attacker. If smaller farms participate in generating bitcoins, they can collectively outnumber larger zombie farms, thus maintaining the integrity of the network.\n",
      "  - context: ['Source: https://satoshi.nakamotoinstitute.org/emails/cryptography/3/\\nFrom: | Satoshi Nakamoto | Subject: | Bitcoin P2P e-cash paper | Date: | November 3, 2008 at 16:23:49 UTC\\n>> As long as honest nodes control the most CPU power on the network,\\n>> they can generate the longest chain and outpace any attackers.\\n>\\n>But they don\\'t.  Bad guys routinely control zombie farms of 100,000\\n>machines or more.  People I know who run a blacklist of spam sending\\n>zombies tell me they often see a million new zombies a day.\\n>\\n>This is the same reason that hashcash can\\'t work on today\\'s Internet\\n>-- the good guys have vastly less computational firepower than the bad\\n>guys.\\nThanks for bringing up that point.\\nI didn\\'t really make that statement as strong as I could have.  The requirement is that the good guys collectively have more CPU power than any single attacker.\\nThere would be many smaller zombie farms that are not big enough to overpower the network, and they could still make money by generating bitcoins.  The smaller farms are then the \"honest nodes\".  (I need a better term than \"honest\")  The more smaller farms resort to generating bitcoins, the higher the bar gets to overpower the network, making larger farms also too small to overpower it so that they may as well generate bitcoins too.  According to the \"long tail\" theory, the small, medium and merely large farms put together should add up to a lot more than the biggest zombie farm.\\nEven if a bad guy does overpower the network, it\\'s not like he\\'s instantly rich.  All he can accomplish is to take back money he himself spent, like bouncing a check.  To exploit it, he would have to buy something from a merchant, wait till it ships, then overpower the network and try to take his money back.  I don\\'t think he could make as much money trying to pull a carding scheme like that as he could by generating bitcoins.  With a zombie farm that big, he could generate more bitcoins than everyone else combined.\\nThe Bitcoin network might actually reduce spam by diverting zombie farms to generating bitcoins instead.\\nSatoshi Nakamoto\\n---------------------------------------------------------------------\\nThe Cryptography Mailing List\\nUnsubscribe by sending \"unsubscribe cryptography\" to majordomo at metzdowd.com\\nBitcoin P2P e-cash paper\\nCryptography Mailing List']\n",
      "  - retrieval context: [\"Source: https://satoshi.nakamotoinstitute.org/posts/bitcointalk/200/\\nQuote from: knightmb on July 15, 2010, 07:37:10 PMOn Windows, the priority of the Coin Generation is still net for normal. If you run BitCoin in Generate Coin mode, then load up something to eat up all the CPU (like CPU hog for example:http://www.microtask.ca/cpuhog.html) you'll see that both BitCoin and CPU hog share the CPU 50/50 instead of CPU Hog taking all the CPU and BitCoin running only on idle/low process. The khash/s is also reduced in half, so further evidence that the threads are not running in a lower than normal prioirty.I was not able to reproduce this.Ãƒâ€šÃ‚\\xa0 I have dual-proc, so I ran two memory hogs.Ãƒâ€šÃ‚\\xa0 Bitcoin got 0% of CPU according to the task manager.Ãƒâ€šÃ‚\\xa0 The khash/sec meter stayed stuck because it couldn't get any CPU to update it.Do you have dual-proc?Ãƒâ€šÃ‚\\xa0 Are you sure you weren't running a single processor hog?\\nRe: 0.3.1 release candidate, please test\\nJuly 15, 2010 at 21:40:34 UTC\\nBitcoinTalk\", \"I think it would be possible for BitDNS to be a completely separate network and separate block chain, yet share CPU power with Bitcoin.Ãƒâ€šÃ‚\\xa0 The only overlap is to make it so miners can search for proof-of-work for both networks simultaneously.The networks wouldn't need any coordination.Ãƒâ€šÃ‚\\xa0 Miners would subscribe to both networks in parallel.Ãƒâ€šÃ‚\\xa0 They would scan SHA such that if they get a hit, they potentially solve both at once.Ãƒâ€šÃ‚\\xa0 A solution may be for just one of the networks if one network has a lower difficulty.I think an external miner could call getwork on both programs and combine the work.Ãƒâ€šÃ‚\\xa0 Maybe call Bitcoin, get work from it, hand it to BitDNS getwork to combine into a combined work.Instead of fragmentation, networks share and augment each other's total CPU power.Ãƒâ€šÃ‚\\xa0 This would solve the problem that if there are multiple networks, they are a danger to each other if the available CPU power gangs up on one.Ãƒâ€šÃ‚\\xa0 Instead, all networks in the world would share combined CPU power, increasing the\", 'Re: Runaway CPU usage for 64bit BitCoin (Linux Client)\\nJuly 14, 2010 at 18:45:53 UTC\\nBitcoinTalk', 'Source: https://satoshi.nakamotoinstitute.org/posts/bitcointalk/204/\\nThe fix for the thread priority level on linux is available in the 0.3.1 release candidate here:http://bitcointalk.org/index.php?topic=383.msg3198#msg3198\\nRe: Runaway CPU usage for 64bit BitCoin (Linux Client)\\nJuly 15, 2010 at 22:22:30 UTC\\nBitcoinTalk']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âœ… Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because the response directly addresses the main idea behind Satoshi Nakamoto's proposal without any irrelevant statements., error: None)\n",
      "  - âœ… Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because there are no contradictions present, indicating that the actual output aligns perfectly with the retrieval context., error: None)\n",
      "  - âŒ Contextual Recall (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 0.00 because the expected output lacks any direct connection to the node(s) in retrieval context, as it fails to reference specific content or details from those sources., error: None)\n",
      "  - âŒ Contextual Precision (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 0.00 because all nodes in the retrieval contexts are irrelevant, as they do not provide any specific information about Satoshi Nakamoto's proposal. For instance, the first node ranks highest but states, \"The first source does not provide any specific information about Satoshi Nakamoto's proposal, only a link,\" indicating a lack of relevant content. Similarly, the second node mentions, \"The second source is also just a link and does not contain any details regarding the main idea of the proposal,\" and this pattern continues with the remaining nodes, which all fail to address the main idea., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the main idea behind Satoshi Nakamoto's proposal?\n",
      "  - actual output: Based on the provided sources, I can summarize the main idea behind Satoshi Nakamoto's proposal as follows:\n",
      "\n",
      "Satoshi Nakamoto proposed a decentralized digital currency system that would enable peer-to-peer transactions without the need for intermediaries such as banks or governments. The system, known as Bitcoin, would use a consensus mechanism called proof-of-work to secure and validate transactions.\n",
      "\n",
      "The key points of his proposal include:\n",
      "\n",
      "* A decentralized network that enables individuals to send and receive value directly\n",
      "* Use of cryptographic techniques to secure transactions and control the creation of new units\n",
      "* A decentralized timestamp server to record transaction history\n",
      "* No central authority or intermediary required for transactions\n",
      "\n",
      "Overall, Satoshi Nakamoto's proposal aimed to create a trustless and censorship-resistant system for peer-to-peer value transfer.\n",
      "  - expected output: The main idea behind Satoshi Nakamoto's proposal is to create a fully peer-to-peer electronic cash system that operates without a trusted third party. This system aims to enable secure and efficient transactions within a large network, addressing concerns such as double spending through mechanisms like Simplified Payment Verification, which minimizes bandwidth requirements for users.\n",
      "  - context: ['Source: https://satoshi.nakamotoinstitute.org/emails/cryptography/2/\\nFrom: | Satoshi Nakamoto | Subject: | Bitcoin P2P e-cash paper | Date: | November 3, 2008 at 01:37:43 UTC\\n>Satoshi Nakamoto wrote:\\n>> I\\'ve been working on a new electronic cash system that\\'s fully\\n>> peer-to-peer, with no trusted third party.\\n>>\\n>> The paper is available at:\\n>> http://www.bitcoin.org/bitcoin.pdf\\n>\\n>We very, very much need such a system, but the way I understand your\\n>proposal, it does not seem to scale to the required size.\\n>\\n>For transferable proof of work tokens to have value, they must have\\n>monetary value.  To have monetary value, they must be transferred within\\n>a very large network - for example a file trading network akin to\\n>bittorrent.\\n>\\n>To detect and reject a double spending event in a timely manner, one\\n>must have most past transactions of the coins in the transaction, which,\\n>  naively implemented, requires each peer to have most past\\n>transactions, or most past transactions that occurred recently. If\\n>hundreds of millions of people are doing transactions, that is a lot of\\n>bandwidth - each must know all, or a substantial part thereof.\\n>\\nLong before the network gets anywhere near as large as that, it would be safe for users to use Simplified Payment Verification (section 8) to check for double spending, which only requires having the chain of block headers, or about 12KB per day.  Only people trying to create new coins would need to run network nodes.  At first, most users would run network nodes, but as the network grows beyond a certain point, it would be left more and more to specialists with server farms of specialized hardware.  A server farm would only need to have one node on the network and the rest of the LAN connects with that one node.\\nThe bandwidth might not be as prohibitive as you think.  A typical transaction would be about 400 bytes (ECC is nicely compact).  Each transaction has to be broadcast twice, so lets say 1KB per transaction.  Visa processed 37 billion transactions in FY2008, or an average of 100 million transactions per day.  That many transactions would take 100GB of bandwidth, or the size of 12 DVD or 2 HD quality movies, or about $18 worth of bandwidth at current prices.\\nIf the network were to get that big, it would take several years, and by then, sending 2 HD movies over the Internet would probably not seem like a big deal.\\nSatoshi Nakamoto\\n---------------------------------------------------------------------\\nThe Cryptography Mailing List\\nUnsubscribe by sending \"unsubscribe cryptography\" to majordomo at metzdowd.com\\nBitcoin P2P e-cash paper\\nCryptography Mailing List']\n",
      "  - retrieval context: ['Source: https://satoshi.nakamotoinstitute.org/posts/p2pfoundation/1/', 'Source: https://satoshi.nakamotoinstitute.org/posts/bitcointalk/258/', 'Source: https://satoshi.nakamotoinstitute.org/posts/p2pfoundation/\\nForum Posts', 'Source: https://satoshi.nakamotoinstitute.org/posts/p2pfoundation/3/']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "Contextual Recall: 0.00% pass rate\n",
      "Contextual Precision: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">âœ“</span> Done ðŸŽ‰! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w1l6609o195326b6z0oyo/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w1l6609o195326b6z0oyo/compa</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w1l6609o195326b6z0oyo/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">re-test-results</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141mâœ“\u001b[0m Done ðŸŽ‰! View results on \n",
       "\u001b]8;id=78609;https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w1l6609o195326b6z0oyo/compare-test-results\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w1l6609o195326b6z0oyo/compa\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=78609;https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w1l6609o195326b6z0oyo/compare-test-results\u001b\\\u001b[4;94mre-test-results\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 2 seconds before next batch...\n",
      "\n",
      "Processing batch 3/3 (test cases 5-5)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Recall Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Recall Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0bc55d650246ccbce172b917875483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âœ… Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because the response directly addresses the question about broadcast transactions without any irrelevant statements., error: None)\n",
      "  - âœ… Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because there are no contradictions present, indicating that the actual output aligns perfectly with the retrieval context., error: None)\n",
      "  - âœ… Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because all sentences in the expected output are directly supported by the information from node(s) in retrieval context, clearly explaining how transactions are handled in the blockchain., error: None)\n",
      "  - âœ… Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because all relevant nodes are ranked higher than the irrelevant nodes. The first node directly addresses the question by stating that 'if a broadcast transaction does not reach all nodes, it is OK, as it will get into the block chain before long,' while the second node supports this by mentioning that 'nodes keep transactions in their working set until they get into a block.' In contrast, the third node discusses re-broadcasting and forks, which is not directly relevant, and the fourth node focuses on lost coins and forks, providing no relevant information about the fate of a broadcast transaction. This clear distinction in relevance justifies the perfect score., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What happens if a broadcast transaction does not reach all nodes?\n",
      "  - actual output: According to Satoshi Nakamoto's response in the email thread, if a broadcast transaction does not reach all nodes, it is still likely to get into the block chain before long. This is because each time a new block is found, there is a 90% chance that the transaction will be included in it, assuming the transaction has reached 90% of nodes. Additionally, if multiple nodes have heard about the transaction, they will keep it in their working set until it gets into a block.\n",
      "  - expected output: If a broadcast transaction does not reach all nodes, it can still be included in the blockchain later. Nodes that receive the transaction will keep it in their working set until it is confirmed in a block. If a transaction reaches 90% of nodes, it has a 90% chance of being included in the next block found.\n",
      "  - context: ['Source: https://satoshi.nakamotoinstitute.org/emails/cryptography/6/\\nFrom: | Satoshi Nakamoto | Subject: | Bitcoin P2P e-cash paper | Date: | November 9, 2008 at 01:58:48 UTC\\nHal Finney wrote:\\n> it is mentioned that if a broadcast transaction does not reach all nodes,\\n> it is OK, as it will get into the block chain before long. How does this\\n> happen - what if the node that creates the \"next\" block (the first node\\n> to find the hashcash collision) did not hear about the transaction,\\n> and then a few more blocks get added also by nodes that did not hear\\n> about that transaction? Do all the nodes that did hear it keep that\\n> transaction around, hoping to incorporate it into a block once they get\\n> lucky enough to be the one which finds the next collision?\\nRight, nodes keep transactions in their working set until they get into a block.  If a transaction reaches 90% of nodes, then each time a new block is found, it has a 90% chance of being in it.\\n> Or for example, what if a node is keeping two or more chains around as\\n> it waits to see which grows fastest, and a block comes in for chain A\\n> which would include a double-spend of a coin that is in chain B? Is that\\n> checked for or not? (This might happen if someone double-spent and two\\n> different sets of nodes heard about the two different transactions with\\n> the same coin.)\\nThat does not need to be checked for.  The transaction in whichever branch ends up getting ahead becomes the valid one, the other is invalid.  If someone tries to double spend like that, one and only one spend will always become valid, the others invalid.\\nReceivers of transactions will normally need to hold transactions for perhaps an hour or more to allow time for this kind of possibility to be resolved.  They can still re-spend the coins immediately, but they should wait before taking an action such as shipping goods.\\n> I also don\\'t understand exactly how double-spending, or cancelling\\n> transactions, is accomplished by a superior attacker who is able to muster\\n> more computing power than all the honest participants. I see that he can\\n> create new blocks and add them to create the longest chain, but how can\\n> he erase or add old transactions in the chain? As the attacker sends out\\n> his new blocks, aren\\'t there consistency checks which honest nodes can\\n> perform, to make sure that nothing got erased? More explanation of this\\n> attack would be helpful, in order to judge the gains to an attacker from\\n> this, versus simply using his computing power to mint new coins honestly.\\nThe attacker isn\\'t adding blocks to the end.  He has to go back and redo the block his transaction is in and all the blocks after it, as well as any new blocks the network keeps adding to the end while he\\'s doing that.  He\\'s rewriting history.  Once his branch is longer, it becomes the new valid one.\\nThis touches on a key point.  Even though everyone present may see the shenanigans going on, there\\'s no way to take advantage of that fact.\\nIt is strictly necessary that the longest chain is always considered the valid one.  Nodes that were present may remember that one branch was there first and got replaced by another, but there would be no way for them to convince those who were not present of this.  We can\\'t have subfactions of nodes that cling to one branch that they think was first, others that saw another branch first, and others that joined later and never saw what happened.  The CPU power proof-of-work vote must have the final say.  The only way for everyone to stay on the same page is to believe that the longest chain is always the valid one, no matter what.\\n> As far as the spending transactions, what checks does the recipient of a\\n> coin have to perform? Does she need to go back through the coin\\'s entire\\n> history of transfers, and make sure that every transaction on the list is\\n> indeed linked into the \"timestamp\" block chain? Or can she just do the\\n> latest one?\\nThe recipient just needs to verify it back to a depth that is sufficiently far back in the block chain, which will often only require a depth of 2 transactions.  All transactions before that can be discarded.\\n> Do the timestamp nodes check transactions, making sure that\\n> the previous transaction on a coin is in the chain, thereby enforcing\\n> the rule that all transactions in the chain represent valid coins?\\nRight, exactly.  When a node receives a block, it checks the signatures of every transaction in it against previous transactions in blocks.  Blocks can only contain', ' transactions that depend on valid transactions in previous blocks or the same block.  Transaction C could depend on transaction B in the same block and B depends on transaction A in an earlier block.\\n> Sorry about all the questions, but as I said this does seem to be a\\n> very promising and original idea, and I am looking forward to seeing\\n> how the concept is further developed. It would be helpful to see a more\\n> process oriented description of the idea, with concrete details of the\\n> data structures for the various objects (coins, blocks, transactions),\\n> the data which is included in messages, and algorithmic descriptions\\n> of the procedures for handling the various events which would occur in\\n> this system. You mentioned that you are working on an implementation,\\n> but I think a more formal, text description of the system would be a\\n> helpful next step.\\nI appreciate your questions.  I actually did this kind of backwards.  I had to write all the code before I could convince myself that I could solve every problem, then I wrote the paper.  I think I will be able to release the code sooner than I could write a detailed spec.  You\\'re already right about most of your assumptions where you filled in the blanks.\\nSatoshi Nakamoto\\n---------------------------------------------------------------------\\nThe Cryptography Mailing List\\nUnsubscribe by sending \"unsubscribe cryptography\" to majordomo at metzdowd.com\\nBitcoin P2P e-cash paper\\nCryptography Mailing List']\n",
      "  - retrieval context: ['Source: https://satoshi.nakamotoinstitute.org/emails/cryptography/6/\\nFrom: | Satoshi Nakamoto | Subject: | Bitcoin P2P e-cash paper | Date: | November 9, 2008 at 01:58:48 UTC\\nHal Finney wrote:\\n> it is mentioned that if a broadcast transaction does not reach all nodes,\\n> it is OK, as it will get into the block chain before long. How does this\\n> happen - what if the node that creates the \"next\" block (the first node\\n> to find the hashcash collision) did not hear about the transaction,\\n> and then a few more blocks get added also by nodes that did not hear\\n> about that transaction? Do all the nodes that did hear it keep that\\n> transaction around, hoping to incorporate it into a block once they get\\n> lucky enough to be the one which finds the next collision?\\nRight, nodes keep transactions in their working set until they get into a block.  If a transaction reaches 90% of nodes, then each time a new block is found, it has a 90% chance of being in it.', \"its many listening nodes, then it alerts that the transaction is bad. Ãƒâ€šÃ‚\\xa0A double-spent transaction wouldn't get very far without one of the listeners hearing it. Ãƒâ€šÃ‚\\xa0The double-spender would have to wait until the listening phase is over, but by then, the payment processor's broadcast has reached most nodes, or is so far ahead in propagating that the double-spender has no hope of grabbing a significant percentage of the remaining nodes.\", \"> broadcast transaction record could easily miss those 3 or 4 nodes\\n> and if it does, and those nodes continue to dominate the chain, the\\n> transaction might never get added.\\nIf you're thinking of it as a CPU-intensive digital signing, then you may be thinking of a race to finish a long operation first and the fastest always winning.\\nThe proof-of-work is a Hashcash style SHA-256 collision finding.  It's a memoryless process where you do millions of hashes a second, with a small chance of finding one each time.  The 3 or 4 fastest nodes' dominance would only be proportional to their share of the total CPU power.  Anyone's chance of finding a solution at any time is proportional to their CPU power.\\nThere will be transaction fees, so nodes will have an incentive to receive and include all the transactions they can.  Nodes will eventually be compensated by transaction fees alone when the total coins created hits the pre-determined ceiling.\\n> Also, the work requirement for adding a link to the chain should\", \"Source: https://satoshi.nakamotoinstitute.org/posts/bitcointalk/405/\\nThat's right.Ãƒâ€šÃ‚\\xa0 You don't need to be re-broadcasting your transactions for it to work.When any node disconnects a fork, it dumps all the transactions from the fork back into the transaction pool to add to the new chain.Ãƒâ€šÃ‚\\xa0 The entire network is making sure to re-integrate your transactions again.Ãƒâ€šÃ‚\\xa0 All you should see is that your number of confirmations starts over from 0.In some types of forks, your transaction would have gotten into both forks already, so you're already good either way.\\nRe: Hypothetical question on lost coins / transfers\\nAugust 19, 2010 at 20:28:50 UTC\\nBitcoinTalk\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "Contextual Recall: 100.00% pass rate\n",
      "Contextual Precision: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">âœ“</span> Done ðŸŽ‰! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w1w3s09ob9532w9475xqp/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w1w3s09ob9532w9475xqp/compa</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w1w3s09ob9532w9475xqp/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">re-test-results</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141mâœ“\u001b[0m Done ðŸŽ‰! View results on \n",
       "\u001b]8;id=868313;https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w1w3s09ob9532w9475xqp/compare-test-results\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w1w3s09ob9532w9475xqp/compa\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=868313;https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w1w3s09ob9532w9475xqp/compare-test-results\u001b\\\u001b[4;94mre-test-results\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Total test results: 5\n",
      "Confident link: https://app.confident-ai.com/project/cmfcas3kc0gmz3zfa5jqdn7ts/evaluation/test-runs/cmg7w1w3s09ob9532w9475xqp/compare-test-results\n"
     ]
    }
   ],
   "source": [
    "from deepeval.models import GPTModel\n",
    "\n",
    "llm_gpt = GPTModel(\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "def run_evaluation(test_cases, batch_size=2, delay_between_batches=2):\n",
    "    \"\"\"Run deepeval evaluation with rate limiting to avoid API spam\"\"\"\n",
    "    metrics = [\n",
    "        AnswerRelevancyMetric(model=llm_gpt),\n",
    "        FaithfulnessMetric(model=llm_gpt),\n",
    "        ContextualRecallMetric(model=llm_gpt),\n",
    "        ContextualPrecisionMetric(model=llm_gpt)\n",
    "    ]\n",
    "\n",
    "    print(f\"Running evaluation on {len(test_cases)} test cases with {len(metrics)} metrics...\")\n",
    "    print(f\"Using batch size: {batch_size}, delay between batches: {delay_between_batches}s\")\n",
    "\n",
    "    # Process in batches to avoid rate limits\n",
    "    all_results = []\n",
    "    for i in range(0, len(test_cases), batch_size):\n",
    "        batch = test_cases[i:i + batch_size]\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        total_batches = (len(test_cases) + batch_size - 1) // batch_size\n",
    "\n",
    "        print(f\"\\nProcessing batch {batch_num}/{total_batches} (test cases {i+1}-{min(i+batch_size, len(test_cases))})...\")\n",
    "\n",
    "        try:\n",
    "            batch_results = evaluate(\n",
    "                test_cases=batch,\n",
    "                metrics=metrics\n",
    "            )\n",
    "            all_results.extend(batch_results.test_results)\n",
    "\n",
    "            if batch_num < total_batches:\n",
    "                print(f\"Waiting {delay_between_batches} seconds before next batch...\")\n",
    "                time.sleep(delay_between_batches)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_num}: {e}\")\n",
    "            # Continue with next batch rather than failing completely\n",
    "            continue\n",
    "\n",
    "    # Create a mock results object with all test results\n",
    "    class MockResults:\n",
    "        def __init__(self, test_results):\n",
    "            self.test_results = test_results\n",
    "            self.confident_link = getattr(batch_results, 'confident_link', None) if 'batch_results' in locals() else None\n",
    "\n",
    "        def __str__(self):\n",
    "            return f\"EvaluationResults(test_results={len(self.test_results)})\"\n",
    "\n",
    "    results = MockResults(all_results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total test results: {len(results.test_results)}\")\n",
    "    if results.confident_link:\n",
    "        print(f\"Confident link: {results.confident_link}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run evaluation (automatically traced with Confident-AI)\n",
    "results = run_evaluation(test_cases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
